{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "We would need the following:\n",
    "1. Python 2.7\n",
    "2. Scipy, Numpy, matplotlib (pip install scipy numpy matplotlib)\n",
    "3. Theano (http://deeplearning.net/software/theano/install.html#install)\n",
    "4. Keras (pip install keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Keras to use Theano as backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create or edit ~/.keras/keras.json\n",
    "2. Type the following into the file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\n",
    "    \"epsilon\": 1e-07, \n",
    "    \"floatx\": \"float32\", \n",
    "    \"image_data_format\": \"channels_last\", \n",
    "    \"backend\": \"theano\",\n",
    "    \"image_dim_ordering\": \"th\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if everything installed correctly  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In command line / terminal, type python\n",
    "```\n",
    "(py2) boonkhailim@skysgdmac035 hotel-download-ui (master) $ python\n",
    "Python 2.7.10 (default, Jul 30 2016, 19:40:32) \n",
    "[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)] on darwin\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>> \n",
    "```\n",
    "then the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are good to start now!\n",
    "\n",
    "Start by importing all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential  # simple linear feed forward network\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten  # core layer in neural network\n",
    "from keras.layers import Convolution2D, MaxPooling2D  # Convolutional NN layers\n",
    "from keras.utils import np_utils  # utility function eg: one hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load mnist data from keras datasets \n",
    "\n",
    "What is mnist? \n",
    "- The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning.\n",
    " *Wikipedia*\n",
    " \n",
    "Good thing is that Keras library already include the datasets, therefore we can save our time from downloading and writing code to separate the data into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print x_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have 60000 training data with 28 x 28 pixels, while on the other hand we have 10000 testing data with the same dimensions as well. mnist data cames in greyscale, thus there isn't a 3 channel (RGB) in the datasets. we can try to plot the first few training data and its corresponding labels to visualize the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 training label: [5 0 4 1 9]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABcCAYAAAB+6068AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntENXFEXxCNqhK4qieATt0BVFUTzC/wNeJ5VfGiBZ1gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110c6ff10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "num_of_image_to_be_plotted = 5\n",
    "fig = plt.figure()\n",
    "for i in range(num_of_image_to_be_plotted):\n",
    "    a=fig.add_subplot(1,num_of_image_to_be_plotted,i+1)\n",
    "    plt.imshow(x_train[i])\n",
    "\n",
    "print \"first 5 training label: {}\".format(y_train[:5])\n",
    "\n",
    "print x_train[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data\n",
    "Before we start to use feed the data to Keras, we need to do few more preprocessing.\n",
    "- Theano backend expect a dimension specifying the depth of the image. eg: RGB image will have a depth of 3. In this case of mnist, since it's a greyscale image therefore we have only 1 channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n",
      "(10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)\n",
    "print x_train.shape\n",
    "print x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After which, we normalize our data to be between 0-1. (existing value is 0 - 255, therefore we divide each value by 255). \n",
    "- I think there is another common normalization / scaling technique is ( x - avgValue ) / stdValue. Can explore it to see if it improve the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')  # convert to float since orignal one is uint8\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on the class label shown above ([5, 0, 4, 1, 9]), we need to convert it into what we call the one-hot-encoded format for Keras. For example, since we have 10 different categories (digit 0 to 9), a one-hot-encodded format for number 5 will be 0,0,0,0,0,1,0,0,0,0. For digit 1 it will be 0,1,0,0,0,0,0,0,0,0. Basically a 1 for the class label, and 0 for the others.\n",
    "- We can do this using the utility function provided by Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print y_train.shape\n",
    "print y_test.shape\n",
    "print y_train[0]  # 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the CNN architecture\n",
    "Now the interesting part to define the state of the art Convolutional Neural Network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare a sequential feed forward neural network\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 32, 26, 26)        320       \n",
      "=================================================================\n",
      "Total params: 320\n",
      "Trainable params: 320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Convolution2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(1, 28, 28)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this add the first input layer into the sequential model. input_shape should match the shape of each image in our datasets, in this case, 1 channel and 28 by 28 pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 32, 26, 26)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 24, 24)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32, 12, 12)        0         \n",
      "=================================================================\n",
      "Total params: 9,568\n",
      "Trainable params: 9,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add one more convolution layer\n",
    "- maxpooling is a way to reduce number of parameters in our model. In this case, a 2 by 2 pooling filter is used. How it works is it will slide through the previous layer and obtain the max of the 4 values\n",
    "- finally, a dropout is added to randomly drop out certain weight to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 32, 26, 26)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 24, 24)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 600,810\n",
      "Trainable params: 600,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- last but not least, we flatten the output of the layers and then connect it a fully-connected / dense layer. \n",
    "- after which we apply dropout again and connect it with another dense layer with 10 outputs (which is our final output layer, since we have 10 categories)\n",
    "- softmax is another kind of activation function which will provide probabilities across different categories (where the highest probability for a category can be used as our final result / prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model\n",
    "Compile the model by declaring the loss function and optimizer we want to use. Keras has a lot of different loss fucntion and optimizer that we can choose from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/1\n",
      "48000/48000 [==============================] - 140s - loss: 0.2391 - acc: 0.9258 - val_loss: 0.0623 - val_acc: 0.9831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11055fa50>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=1, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quite self explanatory. \n",
    "- batch_size is the number of samples before each gradient update (default 32)\n",
    "- epochs is the number of run\n",
    "- validation_split is the percentage of data withheld for validation. Can also use validation_data to supply data as validation sets\n",
    "\n",
    "there is also callbacks for you to apply different callbacks during training, eg: to save model weights during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 161s - loss: 0.0958 - acc: 0.9713 - val_loss: 0.0488 - val_acc: 0.9859\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 169s - loss: 0.0757 - acc: 0.9771 - val_loss: 0.0540 - val_acc: 0.9838\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 171s - loss: 0.0608 - acc: 0.9813 - val_loss: 0.0428 - val_acc: 0.9875\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 170s - loss: 0.0557 - acc: 0.9828 - val_loss: 0.0397 - val_acc: 0.9888\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 169s - loss: 0.0462 - acc: 0.9858 - val_loss: 0.0403 - val_acc: 0.9888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1204f92d0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*train for few more epochs for higher accuracy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 9s     \n",
      "[0.030641492582148929, 0.99060000000000004]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the trained model has a 3% loss, and 99% accuracy :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.89346734e-13   2.09238502e-10   8.17848331e-08   9.23238019e-08\n",
      "    1.45258304e-13   5.48398089e-12   4.88568802e-17   9.99999821e-01\n",
      "    4.05325738e-11   8.92524454e-09]\n",
      " [  1.53522350e-08   3.15411597e-07   9.99999642e-01   1.22998971e-08\n",
      "    3.76165099e-10   2.78656633e-12   7.58394625e-10   3.09659798e-11\n",
      "    5.20637367e-10   2.21382605e-13]\n",
      " [  1.27816833e-08   9.99996364e-01   1.76534385e-07   7.74018893e-09\n",
      "    9.68437689e-07   5.44657119e-08   1.57656110e-08   1.67152552e-06\n",
      "    6.98099825e-07   3.40456303e-08]\n",
      " [  9.99997139e-01   2.85667795e-10   8.04159797e-08   1.10353851e-10\n",
      "    2.51742605e-09   1.50479584e-09   2.23656025e-06   1.40814582e-09\n",
      "    3.63241298e-07   1.95514005e-07]\n",
      " [  7.23103591e-11   1.13826051e-08   2.27217960e-08   9.07505928e-12\n",
      "    9.99984860e-01   4.18973085e-11   1.31882958e-08   2.89477387e-10\n",
      "    5.52255841e-08   1.50384512e-05]]\n",
      "5/5 [==============================] - 0s\n",
      "[7 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "print model.predict(x_test[:5])\n",
    "print model.predict_classes(x_test[:5])  # predict first 5 test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 testing label: [[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABcCAYAAAB+6068AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFE9JREFUeJztnXl4FUW6h9/KThYgkAAxLAEMKqAg4Mo44L5v15VRBr06\nemUcdca5LjzquOIy6rgveGG47gs46rjNKMroKIKAyBYQxICBsBpIkJC15o+vT05OziEkOXuf732e\nPKe7urq6+pfu6q+qvqoy1loURVGU+Ccp2hlQFEVRQoMW6IqiKC5BC3RFURSXoAW6oiiKS9ACXVEU\nxSVoga4oiuIStEBXFEVxCUEV6MaYk4wxK40xq40xN4UqU/GMahIY1cUf1cQf1SQ4TEcHFhljkoHv\ngOOBMuBrYJy1dnnoshdfqCaBUV38UU38UU2CJyWIcw8FVltr1wAYY14FzgT2KH6aSbcZZAVxydgm\nkxxqqKaB+rnW2nzVRMgkh11U1bX1WVFNAuN2XTLJoZqdNNpG1aQFVVRstdbm7y1eMAV6IfBjs/0y\n4LCWkYwxVwBXAGSQyWHm2CAuGdtssmVsYyMbKF3rBCW8JiC6LOGrHc2C/HRRTfRZ2WTLWMHC5kEJ\nr4mHj+2MtXuPFYFOUWvtFGvtKGvtqFTSw325uEA18Uc1CYzq4o9qsmeCKdDXA32a7fd2whKWdDqx\nm+rmQQmvCYguQFqzoITXRTXxJ51ONNLYPCjhNWkvwRToXwPFxpj+xpg04ELgndBkKz7pTC7V7ARI\nU028dCYXIEOfFS+qiT+dyaWRRlSTjtPhAt1aWw9cDfwDKAFet9YuC1XG4pEkk8R+DAcYhGrSRJJJ\nAliHPitNqCb+JJkkMsgE1aTDBNMpirX2feD9EOXFFeSZArAstdaOilYeSu8+AoCGDHFJzR+yBYA5\nw2b6xBv4yaUA5MzrBEDPx74MZ7Z2RFOTGEU1aUEKqVhrB0U7H/GKjhRVFEVxCUFZ6EpsUfFeMQBL\nhz8R8HhdizFkK47+PwBeGlUAwOsfjQGgoWRVmHIYP5iRQwB4750XADjwmasB6HNXWGsxUSe5axcA\nVj4xAPA+I7dsHgnAkovEeG5Y/l0UcqfsDbXQFUVRXIJa6C7AY5l/MfzVgMef2S7W1sNzjgegqJ+0\nqf9z8JsAXJRTDsA9l+QBMOBGtdA3H9IZgHoaAMjckBhr7zb27w3AkrHPAt5a3d09FgAw7OwjAejj\nYgu94egRAFw95XUAni7et0PpVF1wOABdF22VdFeuDkHuWkctdEVRFJegFnqcUn/syKbtT4Y96Wyl\nAvBIhbRzfnqB40CxYTMAgyrmA5CUkQHA5LkHAjApb4mkmVsf1jzHExUHiWVeVl8DQPepc6KZnbCT\n0kcs8/5Twm9FxjprT5TRp92SdwaVzsZTawGoGy92c7fTgstXW1ALXVEUxSXEtIW+7TfiT913vNdq\nWLG5JwC1NWKNFr4iv5ll8jVtXJQYM23uLPSOGk9yvssey3z2GWJ5N6xZGfDc1XccDMDL3R5yQsQi\n6f2hft/t6OEAfH7awwCM+ex3AOzLN1HLUzhZd5u0iY88Sd6bBwo+bzV+9pHS//LjrXJe3mKp1XV6\ne164shgxTKq8U8ccsygk6eV8IzXh8y/7FwCfdu3ddKxh+46A5wSLvsGKoiguQQt0RVEUlxDTTS43\n/O/LAJyTVeENHNgi0lj5Ka3fBcCjW44O6przNvcDIOuhLk1hKbMWBJVmOOj6vLeT7tz5FwNgKioB\nqC8vbfXcy0/5GIDsJJ16tCU/DZZpEAqSMwEonJEazeyEncVXPg5AnW1oU/zZw16SjWHy87efZVDa\ntKqzAEj5JPbelbZSdba4Kz5WKJoc8JYMJitmbofSq8kVn89rclcAMDvnAO9BbXJRFEVRWiOmLfTH\nJl0IwG0Heb87uSXy1as4wACQdtB2AB4YKoNk/lIgX9P3dmUDcGpmYNejaisuRXNrZPmqsRl1csA5\nf98LrmyKO2hWkDcSZto6DLv0Hulkvqzrg06IdNpcXy4DIHI+LpH0Qpu9uOLYiVLzeevnrgBkz5aO\nZbdpkjpbLOtUk9ym+N/UyjzlpXWyCtrZWT8BcH62uMSe/8IUAE4rHBng7NjG0xH+5P2PAvBipdTS\n979F3quO/u+POGFp0HlrL2qhK4qiuISYttCzZsx1fv2PdW6x/3ivsQDcPbpIjv9LXB0fGBt42G5K\ntVgcWYtl2Hv3z2Rq2QPTHDfIUve0nW4fL5b5F78Wy7xLkljmc2rEOlt0t7gxdqqMf9ezjpI8ZD8A\nJvd4BYCpleJiFi73smhRfdahAFxa8AbgbTvfUxv60Fn/A0D+LOlvSd8h8W4eK7bgkvMe84lfdrO4\nM/a+N34mMau4WfrfeqeIC+YffncqAKkVHesPSCnoBcBf+34IQJ2NnN2sFrqiKIpLiGkLvT3Ub9wE\nQNZM+fXYG1kztrV63qbLxXodkiZSPPiTWGpFf13jTTuUGY0CW0dIv4PHMvcwYfblAAx6K3Etcw/r\nj+/us7+gqp+zVe0fOc7w1D4A7n5Y2rpHpdV6jvrE9Xit3PLpOQAccIN4aDRUVvrE22+VDGKbd4Y8\nU4em7wbgg6seAOCEjBua4hZNFkvX1tQEdyMhxDNoEeCNA/8MwPM7DgIg9ePgPHWW3ylLLXtqPRNK\njwOgYfOWoNJtC2qhK4qiuATXWOjtJaWffEWfmCSLQXh6+994VL6m3cvjfzKm2o/Eypyzv2eIv1hT\nw+ZMAOCA678H3OfB0REqB9f57C96QjwfuhL/z0Fjmvc191rmvvz32pMAqLpA/PAHlUmtbU/Phsez\nauJ0aWOff+UjABQky/kLL3ukKe45b8rzZr8t6Uj2w0LSWVubtvdJkf6BqS+LBr3pWPu/pyb04rEy\n9XCNlWdq3cNSm8mq6Zg/e3tQC11RFMUlJKyFvuL3hQAcki7+7Mtqpa202/JdUctTqEgZUATAXfuK\nJ0Ou03a+wGnC7HeX2F0NFRV+5yYaNScfAsDbJ8jowDu3ih91t5mLAWiMTrYixqRNMsVy5eXSh9BQ\n1r7FTYpmiqV761kyluG+Xl+HMHehJzlf/OhvGfSe37Hek4PzzFkxUcYujEqX9+vJisEAZM0Mv2Xu\nQS10RVEUl5BwFnrNqWKRLTz3L06ItJ9dde21AHT6Mv49Pga+vh6Ag9N8v9fjHJ/iQd/GthUVScqO\nkVfgoDSpxUwolamHe/y8Imp5CictR4YuHuFZWq+Dyw4aqeGmJDUGTB9gwx3y2+usjl0ilJhM+T+f\nmOkdX3Do178GoBfBtfHnFf3ks//SD1L7ySNyy/Wpha4oiuISEs5CX3eyfMOyjVjm436QhZMzP/wW\ngHheCrhigvjW3tHTd+EKjx/sATfI6Fn1avGSP1TmImmwYmGmvJ0bzeyEhZVXZTZtt3VWxbZS+l/S\n9j4jf56TfrLfdfb5k/zGQn9E408y99NdW0Y0hf1qoCzN+FmBTOVaX76xXWl6POa8i7RLGVP9VZ6z\nrxa6oiiK0k4SxkJPyskBYPxR/wagslFGtm2ePACA9Jr4bVdOKdwHgKOukd70lvOcz1ku89kMqojf\neww1Kf3FR//B/cQT6LkdYmV1mxb/fuctueWov4csLc9i0lUj5Zl75tKnAsabV+MdlWxqY2esdWNV\nFQD/XL9/U9jnw2XdhfJ3ZQ2Ez589wv/EZmwfLPX47CJphz98n1JJu0UdxEShuq8WuqIoiktIGAt9\n1e1DAHg3TyyKM1fJXBXp78e/1VoySazLt3r5WmJHLzkP0LbzQKy6UizMw53KzG8WykpXfYj8HNbx\nxPI7ZCbBZSc8EfD4zJ3Sbvz0H89rCssoiT3Psdw7vDWIMbePA+BvQ6cDcP+fWq+lzXdmKW1w7GHv\n6FvjE6/v40uAyPYdqIWuKIriElxvoe+4WEawLb5A5m3+vl7mV9h5v7QFplMenYyFkAVn+PrUe+gy\nUWyDeh0R6kdjn90++9XbM/YQUwHvCkf3FsxsNd709TIfesbfY88q92HekqbNLqfI7/ix1wCwvbj1\ntXa7P+drwa9/U2r/Cw6b7hPuaa+PJGqhK4qiuIS9WujGmD7A80BPxE17irX2UWNMN+A1oAgoBc63\n1saMKejx/Lju1tcASDdyqxd+Ox6A/A863na+2+5iGV9Ty27AUEh/+ppi6mwtS/gKYKgx5iOirEld\nT+m1T60tbDVewxaZj8MzX7VJFwslOT/PN15+16btVden+Ryr37adrU/PoGHHTjCGgdv60y99CLt3\nbI0pTTw8ddiLPvuFH7Rtbc320tqzAhQbY1YR5vcn2XhbcVuO5Kz81eE++3fcORWAozv51mA853n9\nywPrZY9Zv9f8tKbJLqqIhCaBSJ69EIDus9t3XnWpeNBxmG+4Z61S88WiIHPWdtpiodcD11trBwOH\nA781xgwGbgJmWWuLgVnOfkJgMBRzEEeYEzmEoynje3baSkpZQTd6ACwlwTQhKYnci05mn/t+T6/b\nrmJdbQk7GyoSWxNaf1aAKn1/fDVJJpVE1CRU7NVCt9aWgzQ0W2urjDElQCFwJjDWifb/wGzgxrDk\nsh2YFLmlYe+WAXBetqxY9FJVDwB63irfsGB6ntNNJ9KReZ9TTCqZNocaqtnCBkYyhtXiKRF1Td6b\nMa1N8Y78Rnr5t26SlVpz86Xtb+7Ilzt87W6P9uH70flUPhdbmuw+XdbU/EWGp403vN1IrT0rgGc5\nrbDqct9r5zZtn99snnKAz/78JOA/grRuDz7Ue1t7tJiFe81Pa5qk0lTzi/qz0mYc55akFvZxJC1z\nD+1qQzfGFAEHA3OBnk5hD7ARaZIJdM4Vxpj5xpj5dcTOElShotr+TBXb6UI3aqkh3XTyHEpYTUp/\nrKNmw3oy+vZTTZrR8lkBPKtqJKwuLTVpVigmrCbB0GbzxBiTDcwErrPWVhrj9bm01lpjAo+LstZO\nAaYAdDbdwj92apisGnJXjxd8gp+cLH6xXb8N3UjAelvPYuawH8NJMak+E8FEUpMzl18EwKyhMzp0\n/pcHv9Lq8V3S3kud9a/XnLL4EgB2LJL29saaGtY/9xSDBpxO3pwUyprFjYXnZN0ZkrSnT+XOrTK7\nYvbbzrqXYbpuNJ+VAa95V+eZd7HvGqDtxTMCdMrGMQBUTBS/9P1/aP9Yh1h5f4LGyUHLkaLRoE0W\nujEmFSnMX7LWvukEbzLGFDjHC4DN4clibNJoG1nMHHrRlx5GOh3TSKfGykIZiaiJbWig/OXpZA8f\nQV6voYBqAnt+VoBUSExd9qSJp1BMRE1CwV4LdCOm+FSgxFr7cLND7wATnO0JwNuhz15sYq1lOfPJ\nIod+ZlBTeD77UM5az27CabL5zddIy+9J7i/GNIUnsibQ+rMCdHd2E0qX1jSpo2nN04TSJFS0pcll\nNDAeWGKM8bTyTwLuA143xlwGrAXOD08W20byYHkwrnjV9xkYPO23ABS98FXIrrWDbWxkHdl04Sv7\nEQD7MpR+7NfkogdsJ0KadDrxBwCGTL4aALuH/2rO/jIB/546O4d8fqmcvy7LJ3zAjJ2y0Wwwhodc\nZ2EEY+fwPQvIpgsbv1zMRqKrSUuSO0uH742j3/cJf/mDXwIwoD48k3K19qys5bvOjoteWN8fz4LO\nALf94XIAfjxdLOHvTn62XWlNnCadn33u8SzX1n6vwtY0KWMNkdAklDRm+Da1bGmIXrt+W7xc/k3L\nSQq8HBva7MQHXU0ex3FuwGMjGcPHdsZSa+1xEc5WVFFNAtOaLli+s9aOimyOok9rmmTaHCrtT8UR\nzpJrcM3Q/xUTZWGC0zMrfcJ7z3aqcDb6fSfhpv+ktlmZpzEy8PksDmV2YopGZ9DU8l0y4Oy49VKO\nFk9eBiTOxGWd3hZ3zUFORfaX46QGm3rJJgA+HCID8U5YeiEAjdPF3dc6Jl3Roi1A4ujVFl486RkA\nSmrFUh83/QYA+hLcotMdQYf+K4qiuIS4t9A9A0Vmne5Zdi1zz5GVhMUzrcFKp4EjzemoTXRLs/Mr\nTt+S47l6NvI+ZbHGibHGJ36i6xWIO384A4CfnxJvnb4zI2+Ze1ALXVEUxSXEvYW+YbRMEtQ3xdcy\n9wz1T62UNnT3t6ArihIVjpXhc1k+w+iig1roiqIoLiHuLfSW3LttMABzTiwCwJb7+04riqK4EbXQ\nFUVRXELcW+gDbhLf61NuGtHiyMbIZ0ZRFCWKqIWuKIriEoyN4AhKY8wW4Gdg697ixgl5BL6Xftba\n/LYk4EJNILAuqkkQmoArdVFN/AmqTIlogQ5gjJnvlvkrQnUvbtIEQnM/qkl404kFVBN/gr0XbXJR\nFEVxCVqgK4qiuIRoFOhTonDNcBGqe3GTJhCa+1FNwptOLKCa+BPUvUS8DV1RFEUJD9rkoiiK4hIi\nVqAbY04yxqw0xqw2xtwUqeuGCmNMH2PMp8aY5caYZcaYa53w240x640xi5y/U9qZbtzqopr4o5oE\nJhy6qCYBsNaG/Q9IBr4HBgBpwLfA4EhcO4T3UACMcLZzgO+AwcDtwB8TURfVRDWJli6qSeC/SFno\nhwKrrbVrrLW1wKvAmRG6dkiw1pZbaxc621VACVAYZLJxrYtq4o9qEpgw6KKaBCBSBXoh8GOz/TKC\nf8ijhjGmCDgYmOsEXW2MWWyMmWaMyW1HUq7RRTXxRzUJTIh0UU0CoJ2i7cQYkw3MBK6z1lYCTwMD\ngeFAOfBQK6e7EtXEH9UkMKqLP6HUJFIF+nqgT7P93k5YXGGMSUWEf8la+yaAtXaTtbbBWtsIPAfO\nooxtI+51UU38UU0CE2JdVJMARKpA/xooNsb0N8akARcC70To2iHBGGOAqUCJtfbhZuEFzaKdDSxt\nR7JxrYtq4o9qEpgw6KKaBCAi86Fba+uNMVcD/0B6p6dZa5dF4tohZDQwHlhijFnkhE0CxhljhiPL\nlpYCV7Y1QRfoopr4o5oEJqS6qCaB0ZGiiqIoLkE7RRVFUVyCFuiKoiguQQt0RVEUl6AFuqIoikvQ\nAl1RFMUlaIGuKIriErRAVxRFcQlaoCuKoriE/wDfexCE06mMoAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ea5a1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_of_image_to_be_plotted = 5\n",
    "fig = plt.figure()\n",
    "for i in range(num_of_image_to_be_plotted):\n",
    "    a=fig.add_subplot(1,num_of_image_to_be_plotted,i+1)\n",
    "    plt.imshow(x_test[i][0])\n",
    "\n",
    "print \"first 5 testing label: {}\".format(y_test[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
